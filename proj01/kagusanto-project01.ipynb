{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 01 - Addition Two Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief summary to the problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given two numbers $x$ and $y$ with a range from 0 to 100, use deep learning to predict the $z$, the result of adding $x$ and $y$ using classification method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $x = [0\\ldotp\\ldotp 100]$ and $y = [0\\ldotp\\ldotp 100]$, the minimum value of $z$ is **0** and the maximum value is **200**. Therefore, there are **200** number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the problem challenging, we need to take out some integers from the possible value in both $x$ and $y$ lists, for example **28, 51, 73**, from the training data. And use them later in the prediction. The idea is to test if the *trained network* can generalize well to the unseen dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code, starting with the import parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from random import shuffle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_range1, max_range2 = 100, 100 # the maximum value of x and y\n",
    "test_list = [28, 51, 73] # the list of the numbers to be taken out from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = max_range1 + max_range2 + 1 # output class\n",
    "num_data = 500000 # number of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We generate the training data using random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_random_data(seed = 8):\n",
    "    z = []    \n",
    "    seed = 8\n",
    "    np.random.seed(seed)\n",
    "    t = [] # validation data\n",
    "    for i in range(0,num_data):\n",
    "        v1 = np.random.randint(0,max_range1+1)\n",
    "        v2 = np.random.randint(0,max_range2+1)\n",
    "        if v1 in test_list or v2 in test_list:\n",
    "            t.append((v1,v2,v1+v2))\n",
    "            continue\n",
    "        z.append((v1,v2,v1+v2))\n",
    "    z = z + t\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = gen_random_data()\n",
    "\n",
    "# set the dataset as numpy array\n",
    "x=np.array( [v[0:2] for v in z] ).astype('float32')\n",
    "y=np.array( [v[2:][0] for v in z] ).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to normalize the training data to be within the range of $[0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "x = x / max_range1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hot encoding\n",
    "yy=to_categorical(y, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We set 80$\\%$ for training data and 20$\\%$ for validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train and validation data\n",
    "n_train = int( 0.8 * num_data ) \n",
    "x_train = x[0:n_train,]\n",
    "x_test = x[n_train:,]\n",
    "y_train = yy[0:n_train,] \n",
    "y_test = yy[n_train:,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup hyperparameter for dense layer\n",
    "n_input = 2 # x + y -- two input dataset\n",
    "n_hidden_1 = 200 # layer - 1\n",
    "n_hidden_2 = 150 # layer - 2\n",
    "n_hidden_3 = 100 # layer - 3\n",
    "n_hidden_4 = 50 # layer - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Inp = Input(shape=(2,))\n",
    "x = Dense( n_hidden_1, activation = 'relu', name = 'Dense_1')(Inp)\n",
    "x = Dense( n_hidden_2, activation = 'relu', name = 'Dense_2')(x)\n",
    "x = Dense( n_hidden_3, activation = 'relu', name = 'Dense_3')(x)\n",
    "x = Dense( n_hidden_4, activation = 'relu', name = 'Dense_4')(x)\n",
    "output = Dense( n_classes, activation = 'softmax', name = 'Dense_out')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Model(Inp, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 200)               600       \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "Dense_3 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "Dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Dense_out (Dense)            (None, 201)               10251     \n",
      "=================================================================\n",
      "Total params: 61,151\n",
      "Trainable params: 61,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_epochs = 200\n",
    "batch_sizes = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a function to allow changing the learning rate at different stage of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    if epoch<20:\n",
    "        return 0.002\n",
    "    if epoch<60:\n",
    "        return 0.001\n",
    "    return 0.0005\n",
    "\n",
    "lrate=LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam()\n",
    "model.compile( loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400000 samples, validate on 100000 samples\n",
      "Epoch 1/200\n",
      "400000/400000 [==============================] - 6s - loss: 5.2737 - acc: 0.0079 - val_loss: 5.1966 - val_acc: 0.0095\n",
      "Epoch 2/200\n",
      "400000/400000 [==============================] - 6s - loss: 5.1543 - acc: 0.0090 - val_loss: 5.0723 - val_acc: 0.0119\n",
      "Epoch 3/200\n",
      "400000/400000 [==============================] - 6s - loss: 5.0466 - acc: 0.0107 - val_loss: 4.9635 - val_acc: 0.0094\n",
      "Epoch 4/200\n",
      "400000/400000 [==============================] - 5s - loss: 4.9291 - acc: 0.0107 - val_loss: 4.8175 - val_acc: 0.0141\n",
      "Epoch 5/200\n",
      "400000/400000 [==============================] - 5s - loss: 4.7122 - acc: 0.0142 - val_loss: 4.4949 - val_acc: 0.0188\n",
      "Epoch 6/200\n",
      "400000/400000 [==============================] - 5s - loss: 4.2896 - acc: 0.0331 - val_loss: 3.9522 - val_acc: 0.0658\n",
      "Epoch 7/200\n",
      "400000/400000 [==============================] - 5s - loss: 3.7538 - acc: 0.0752 - val_loss: 3.4846 - val_acc: 0.0850\n",
      "Epoch 8/200\n",
      "400000/400000 [==============================] - 5s - loss: 3.3761 - acc: 0.1097 - val_loss: 3.1799 - val_acc: 0.1505\n",
      "Epoch 9/200\n",
      "400000/400000 [==============================] - 6s - loss: 3.1104 - acc: 0.1495 - val_loss: 2.9827 - val_acc: 0.1190\n",
      "Epoch 10/200\n",
      "400000/400000 [==============================] - 6s - loss: 2.9352 - acc: 0.1528 - val_loss: 2.8265 - val_acc: 0.1808\n",
      "Epoch 11/200\n",
      "400000/400000 [==============================] - 5s - loss: 2.7872 - acc: 0.1894 - val_loss: 2.6750 - val_acc: 0.2050\n",
      "Epoch 12/200\n",
      "400000/400000 [==============================] - 6s - loss: 2.6647 - acc: 0.2173 - val_loss: 2.5769 - val_acc: 0.2389\n",
      "Epoch 13/200\n",
      "400000/400000 [==============================] - 6s - loss: 2.5700 - acc: 0.2334 - val_loss: 2.4890 - val_acc: 0.2485\n",
      "Epoch 14/200\n",
      "400000/400000 [==============================] - 6s - loss: 2.4917 - acc: 0.2477 - val_loss: 2.4161 - val_acc: 0.2579\n",
      "Epoch 15/200\n",
      "400000/400000 [==============================] - 5s - loss: 2.4354 - acc: 0.2691 - val_loss: 2.3606 - val_acc: 0.3020\n",
      "Epoch 16/200\n",
      "400000/400000 [==============================] - 5s - loss: 2.3689 - acc: 0.2840 - val_loss: 2.2898 - val_acc: 0.2780\n",
      "Epoch 17/200\n",
      "400000/400000 [==============================] - 5s - loss: 2.3209 - acc: 0.2731 - val_loss: 2.2474 - val_acc: 0.3454\n",
      "Epoch 18/200\n",
      "400000/400000 [==============================] - 6s - loss: 2.2655 - acc: 0.2981 - val_loss: 2.2013 - val_acc: 0.2970\n",
      "Epoch 19/200\n",
      "400000/400000 [==============================] - 7s - loss: 2.2212 - acc: 0.3067 - val_loss: 2.1755 - val_acc: 0.3243\n",
      "Epoch 20/200\n",
      "400000/400000 [==============================] - 5s - loss: 2.2230 - acc: 0.3078 - val_loss: 2.1489 - val_acc: 0.3454\n",
      "Epoch 21/200\n",
      "400000/400000 [==============================] - 6s - loss: 2.1348 - acc: 0.3893 - val_loss: 2.0725 - val_acc: 0.3968\n",
      "Epoch 22/200\n",
      "400000/400000 [==============================] - 5s - loss: 2.0853 - acc: 0.4220 - val_loss: 2.0347 - val_acc: 0.4985\n",
      "Epoch 23/200\n",
      "400000/400000 [==============================] - 5s - loss: 2.0550 - acc: 0.4584 - val_loss: 2.0115 - val_acc: 0.4832\n",
      "Epoch 24/200\n",
      "400000/400000 [==============================] - 5s - loss: 2.0315 - acc: 0.4867 - val_loss: 1.9899 - val_acc: 0.4754\n",
      "Epoch 25/200\n",
      "400000/400000 [==============================] - 6s - loss: 2.0091 - acc: 0.4736 - val_loss: 1.9666 - val_acc: 0.5050\n",
      "Epoch 26/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.9877 - acc: 0.4828 - val_loss: 1.9462 - val_acc: 0.5393\n",
      "Epoch 27/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.9668 - acc: 0.5016 - val_loss: 1.9281 - val_acc: 0.4683\n",
      "Epoch 28/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.9476 - acc: 0.5066 - val_loss: 1.9055 - val_acc: 0.4833\n",
      "Epoch 29/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.9264 - acc: 0.5016 - val_loss: 1.8868 - val_acc: 0.5350\n",
      "Epoch 30/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.9097 - acc: 0.5084 - val_loss: 1.8673 - val_acc: 0.4816\n",
      "Epoch 31/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.8913 - acc: 0.4844 - val_loss: 1.8483 - val_acc: 0.5129\n",
      "Epoch 32/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.8734 - acc: 0.4895 - val_loss: 1.8364 - val_acc: 0.5070\n",
      "Epoch 33/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.8548 - acc: 0.5145 - val_loss: 1.8159 - val_acc: 0.5010\n",
      "Epoch 34/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.8413 - acc: 0.4924 - val_loss: 1.8075 - val_acc: 0.4806\n",
      "Epoch 35/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.8240 - acc: 0.4965 - val_loss: 1.7962 - val_acc: 0.4830\n",
      "Epoch 36/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.8066 - acc: 0.4987 - val_loss: 1.7725 - val_acc: 0.4929\n",
      "Epoch 37/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.7906 - acc: 0.5065 - val_loss: 1.7530 - val_acc: 0.5184\n",
      "Epoch 38/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.7783 - acc: 0.5208 - val_loss: 1.7325 - val_acc: 0.5360\n",
      "Epoch 39/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.7641 - acc: 0.5069 - val_loss: 1.7199 - val_acc: 0.5375\n",
      "Epoch 40/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.7436 - acc: 0.5306 - val_loss: 1.7028 - val_acc: 0.5344\n",
      "Epoch 41/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.7348 - acc: 0.5210 - val_loss: 1.6937 - val_acc: 0.5311\n",
      "Epoch 42/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.7206 - acc: 0.5334 - val_loss: 1.6805 - val_acc: 0.5607\n",
      "Epoch 43/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.7101 - acc: 0.5336 - val_loss: 1.6611 - val_acc: 0.5709\n",
      "Epoch 44/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.6958 - acc: 0.5375 - val_loss: 1.6670 - val_acc: 0.5702\n",
      "Epoch 45/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.6804 - acc: 0.5467 - val_loss: 1.6599 - val_acc: 0.5246\n",
      "Epoch 46/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.6728 - acc: 0.5401 - val_loss: 1.6308 - val_acc: 0.5401\n",
      "Epoch 47/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.6545 - acc: 0.5509 - val_loss: 1.6102 - val_acc: 0.5708\n",
      "Epoch 48/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.6433 - acc: 0.5476 - val_loss: 1.5938 - val_acc: 0.6030\n",
      "Epoch 49/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.6317 - acc: 0.5481 - val_loss: 1.5932 - val_acc: 0.5851\n",
      "Epoch 50/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.6359 - acc: 0.5554 - val_loss: 1.6005 - val_acc: 0.5501\n",
      "Epoch 51/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.6368 - acc: 0.5365 - val_loss: 1.5968 - val_acc: 0.5720\n",
      "Epoch 52/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.6207 - acc: 0.5461 - val_loss: 1.5752 - val_acc: 0.5460\n",
      "Epoch 53/200\n",
      "400000/400000 [==============================] - 9s - loss: 1.6033 - acc: 0.5470 - val_loss: 1.5746 - val_acc: 0.5581\n",
      "Epoch 54/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.5888 - acc: 0.5603 - val_loss: 1.5550 - val_acc: 0.5884\n",
      "Epoch 55/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.5738 - acc: 0.5874 - val_loss: 1.5429 - val_acc: 0.5598\n",
      "Epoch 56/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.5619 - acc: 0.5786 - val_loss: 1.5138 - val_acc: 0.6293\n",
      "Epoch 57/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.5502 - acc: 0.5851 - val_loss: 1.5084 - val_acc: 0.5726\n",
      "Epoch 58/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.5442 - acc: 0.5693 - val_loss: 1.5018 - val_acc: 0.5939\n",
      "Epoch 59/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.5273 - acc: 0.5787 - val_loss: 1.4896 - val_acc: 0.5962\n",
      "Epoch 60/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.5177 - acc: 0.6037 - val_loss: 1.4739 - val_acc: 0.6168\n",
      "Epoch 61/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4904 - acc: 0.6523 - val_loss: 1.4568 - val_acc: 0.6435\n",
      "Epoch 62/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4803 - acc: 0.6656 - val_loss: 1.4456 - val_acc: 0.6547\n",
      "Epoch 63/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4738 - acc: 0.6732 - val_loss: 1.4424 - val_acc: 0.6803\n",
      "Epoch 64/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4688 - acc: 0.6755 - val_loss: 1.4352 - val_acc: 0.7252\n",
      "Epoch 65/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4635 - acc: 0.6864 - val_loss: 1.4301 - val_acc: 0.7032\n",
      "Epoch 66/200\n",
      "400000/400000 [==============================] - 8s - loss: 1.4585 - acc: 0.6784 - val_loss: 1.4296 - val_acc: 0.6838\n",
      "Epoch 67/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4539 - acc: 0.6808 - val_loss: 1.4202 - val_acc: 0.6878\n",
      "Epoch 68/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4482 - acc: 0.6933 - val_loss: 1.4158 - val_acc: 0.7049\n",
      "Epoch 69/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.4434 - acc: 0.6823 - val_loss: 1.4103 - val_acc: 0.7064\n",
      "Epoch 70/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4382 - acc: 0.6842 - val_loss: 1.4041 - val_acc: 0.6962\n",
      "Epoch 71/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.4327 - acc: 0.6904 - val_loss: 1.4005 - val_acc: 0.7126\n",
      "Epoch 72/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.4292 - acc: 0.6818 - val_loss: 1.3977 - val_acc: 0.6724\n",
      "Epoch 73/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.4258 - acc: 0.6638 - val_loss: 1.3881 - val_acc: 0.6910\n",
      "Epoch 74/200\n",
      "400000/400000 [==============================] - 10s - loss: 1.4187 - acc: 0.6744 - val_loss: 1.3875 - val_acc: 0.6762\n",
      "Epoch 75/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4142 - acc: 0.6699 - val_loss: 1.3801 - val_acc: 0.6951\n",
      "Epoch 76/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4108 - acc: 0.6897 - val_loss: 1.3771 - val_acc: 0.7182\n",
      "Epoch 77/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.4056 - acc: 0.6842 - val_loss: 1.3724 - val_acc: 0.6688\n",
      "Epoch 78/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3993 - acc: 0.6909 - val_loss: 1.3651 - val_acc: 0.6893\n",
      "Epoch 79/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3950 - acc: 0.6923 - val_loss: 1.3608 - val_acc: 0.6936\n",
      "Epoch 80/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3902 - acc: 0.6904 - val_loss: 1.3597 - val_acc: 0.6626\n",
      "Epoch 81/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3869 - acc: 0.6836 - val_loss: 1.3524 - val_acc: 0.6956\n",
      "Epoch 82/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3818 - acc: 0.6736 - val_loss: 1.3490 - val_acc: 0.7133\n",
      "Epoch 83/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3769 - acc: 0.6908 - val_loss: 1.3418 - val_acc: 0.7232\n",
      "Epoch 84/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3714 - acc: 0.6844 - val_loss: 1.3394 - val_acc: 0.7006\n",
      "Epoch 85/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3653 - acc: 0.7082 - val_loss: 1.3338 - val_acc: 0.7061\n",
      "Epoch 86/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3625 - acc: 0.7024 - val_loss: 1.3289 - val_acc: 0.6913\n",
      "Epoch 87/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3584 - acc: 0.6931 - val_loss: 1.3254 - val_acc: 0.6949\n",
      "Epoch 88/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3536 - acc: 0.6826 - val_loss: 1.3201 - val_acc: 0.6752\n",
      "Epoch 89/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3488 - acc: 0.6780 - val_loss: 1.3162 - val_acc: 0.7280\n",
      "Epoch 90/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3457 - acc: 0.6767 - val_loss: 1.3112 - val_acc: 0.6809\n",
      "Epoch 91/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3407 - acc: 0.6606 - val_loss: 1.3073 - val_acc: 0.6983\n",
      "Epoch 92/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3379 - acc: 0.6732 - val_loss: 1.3058 - val_acc: 0.7079\n",
      "Epoch 93/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3315 - acc: 0.6781 - val_loss: 1.2965 - val_acc: 0.7108\n",
      "Epoch 94/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3277 - acc: 0.6904 - val_loss: 1.2921 - val_acc: 0.6813\n",
      "Epoch 95/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3231 - acc: 0.6843 - val_loss: 1.2886 - val_acc: 0.7008\n",
      "Epoch 96/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.3173 - acc: 0.6882 - val_loss: 1.2842 - val_acc: 0.7286\n",
      "Epoch 97/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.3119 - acc: 0.7007 - val_loss: 1.2794 - val_acc: 0.7036\n",
      "Epoch 98/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3065 - acc: 0.7009 - val_loss: 1.2772 - val_acc: 0.7624\n",
      "Epoch 99/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3048 - acc: 0.7083 - val_loss: 1.2750 - val_acc: 0.6800\n",
      "Epoch 100/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.3007 - acc: 0.6780 - val_loss: 1.2695 - val_acc: 0.6636\n",
      "Epoch 101/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2967 - acc: 0.6748 - val_loss: 1.2661 - val_acc: 0.7056\n",
      "Epoch 102/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2937 - acc: 0.6814 - val_loss: 1.2623 - val_acc: 0.7076\n",
      "Epoch 103/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2883 - acc: 0.6948 - val_loss: 1.2528 - val_acc: 0.7227\n",
      "Epoch 104/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2843 - acc: 0.6946 - val_loss: 1.2500 - val_acc: 0.6729\n",
      "Epoch 105/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2793 - acc: 0.6847 - val_loss: 1.2481 - val_acc: 0.6892\n",
      "Epoch 106/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2740 - acc: 0.6827 - val_loss: 1.2449 - val_acc: 0.7394\n",
      "Epoch 107/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2701 - acc: 0.7047 - val_loss: 1.2344 - val_acc: 0.7166\n",
      "Epoch 108/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2649 - acc: 0.7064 - val_loss: 1.2333 - val_acc: 0.7339\n",
      "Epoch 109/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2607 - acc: 0.7060 - val_loss: 1.2287 - val_acc: 0.7048\n",
      "Epoch 110/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2587 - acc: 0.6930 - val_loss: 1.2259 - val_acc: 0.6883\n",
      "Epoch 111/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2543 - acc: 0.6921 - val_loss: 1.2225 - val_acc: 0.7183\n",
      "Epoch 112/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2493 - acc: 0.6997 - val_loss: 1.2153 - val_acc: 0.7556\n",
      "Epoch 113/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2455 - acc: 0.7066 - val_loss: 1.2128 - val_acc: 0.7138\n",
      "Epoch 114/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2419 - acc: 0.7067 - val_loss: 1.2076 - val_acc: 0.7193\n",
      "Epoch 115/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2368 - acc: 0.6955 - val_loss: 1.2029 - val_acc: 0.7098\n",
      "Epoch 116/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2324 - acc: 0.7096 - val_loss: 1.1991 - val_acc: 0.7194\n",
      "Epoch 117/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2270 - acc: 0.7083 - val_loss: 1.1979 - val_acc: 0.7445\n",
      "Epoch 118/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2242 - acc: 0.7322 - val_loss: 1.1966 - val_acc: 0.6973\n",
      "Epoch 119/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.2225 - acc: 0.7206 - val_loss: 1.1893 - val_acc: 0.7150\n",
      "Epoch 120/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2164 - acc: 0.7219 - val_loss: 1.1842 - val_acc: 0.7494\n",
      "Epoch 121/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2130 - acc: 0.7241 - val_loss: 1.1802 - val_acc: 0.7500\n",
      "Epoch 122/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2094 - acc: 0.7280 - val_loss: 1.1742 - val_acc: 0.7064\n",
      "Epoch 123/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2064 - acc: 0.7126 - val_loss: 1.1782 - val_acc: 0.6867\n",
      "Epoch 124/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2067 - acc: 0.7008 - val_loss: 1.1705 - val_acc: 0.7111\n",
      "Epoch 125/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.2001 - acc: 0.7136 - val_loss: 1.1720 - val_acc: 0.7357\n",
      "Epoch 126/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1984 - acc: 0.7073 - val_loss: 1.1639 - val_acc: 0.7141\n",
      "Epoch 127/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1940 - acc: 0.6939 - val_loss: 1.1626 - val_acc: 0.6920\n",
      "Epoch 128/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1890 - acc: 0.6863 - val_loss: 1.1589 - val_acc: 0.6907\n",
      "Epoch 129/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1866 - acc: 0.7192 - val_loss: 1.1552 - val_acc: 0.7256\n",
      "Epoch 130/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1825 - acc: 0.7179 - val_loss: 1.1488 - val_acc: 0.7243\n",
      "Epoch 131/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1753 - acc: 0.7226 - val_loss: 1.1467 - val_acc: 0.7276\n",
      "Epoch 132/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1732 - acc: 0.7245 - val_loss: 1.1367 - val_acc: 0.7382\n",
      "Epoch 133/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1720 - acc: 0.7124 - val_loss: 1.1379 - val_acc: 0.7494\n",
      "Epoch 134/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.1645 - acc: 0.7396 - val_loss: 1.1329 - val_acc: 0.7204\n",
      "Epoch 135/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1653 - acc: 0.7133 - val_loss: 1.1394 - val_acc: 0.7398\n",
      "Epoch 136/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1638 - acc: 0.7188 - val_loss: 1.1277 - val_acc: 0.7487\n",
      "Epoch 137/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1613 - acc: 0.7044 - val_loss: 1.1290 - val_acc: 0.7294\n",
      "Epoch 138/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1556 - acc: 0.7312 - val_loss: 1.1207 - val_acc: 0.7155\n",
      "Epoch 139/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1517 - acc: 0.7195 - val_loss: 1.1200 - val_acc: 0.7139\n",
      "Epoch 140/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1483 - acc: 0.7163 - val_loss: 1.1176 - val_acc: 0.7397\n",
      "Epoch 141/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1461 - acc: 0.7212 - val_loss: 1.1114 - val_acc: 0.7257\n",
      "Epoch 142/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1420 - acc: 0.7242 - val_loss: 1.1076 - val_acc: 0.7264\n",
      "Epoch 143/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1376 - acc: 0.7310 - val_loss: 1.1069 - val_acc: 0.7254\n",
      "Epoch 144/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1334 - acc: 0.7408 - val_loss: 1.1018 - val_acc: 0.7887\n",
      "Epoch 145/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1325 - acc: 0.7206 - val_loss: 1.0996 - val_acc: 0.6840\n",
      "Epoch 146/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1298 - acc: 0.7047 - val_loss: 1.1056 - val_acc: 0.6861\n",
      "Epoch 147/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1296 - acc: 0.7008 - val_loss: 1.0943 - val_acc: 0.7209\n",
      "Epoch 148/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1232 - acc: 0.7265 - val_loss: 1.0877 - val_acc: 0.7418\n",
      "Epoch 149/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1220 - acc: 0.7187 - val_loss: 1.0893 - val_acc: 0.7531\n",
      "Epoch 150/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1185 - acc: 0.7201 - val_loss: 1.0835 - val_acc: 0.7093\n",
      "Epoch 151/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.1119 - acc: 0.7200 - val_loss: 1.0811 - val_acc: 0.7186\n",
      "Epoch 152/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1113 - acc: 0.7171 - val_loss: 1.0798 - val_acc: 0.7248\n",
      "Epoch 153/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1068 - acc: 0.7175 - val_loss: 1.0765 - val_acc: 0.7406\n",
      "Epoch 154/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.1015 - acc: 0.7356 - val_loss: 1.0698 - val_acc: 0.7836\n",
      "Epoch 155/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0989 - acc: 0.7339 - val_loss: 1.0647 - val_acc: 0.7509\n",
      "Epoch 156/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0956 - acc: 0.7282 - val_loss: 1.0631 - val_acc: 0.7306\n",
      "Epoch 157/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0928 - acc: 0.7337 - val_loss: 1.0625 - val_acc: 0.7422\n",
      "Epoch 158/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0907 - acc: 0.7273 - val_loss: 1.0619 - val_acc: 0.7109\n",
      "Epoch 159/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0895 - acc: 0.7253 - val_loss: 1.0547 - val_acc: 0.7259\n",
      "Epoch 160/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0878 - acc: 0.7330 - val_loss: 1.0553 - val_acc: 0.7401\n",
      "Epoch 161/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0851 - acc: 0.7278 - val_loss: 1.0509 - val_acc: 0.7163\n",
      "Epoch 162/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0790 - acc: 0.7352 - val_loss: 1.0516 - val_acc: 0.7311\n",
      "Epoch 163/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0802 - acc: 0.7244 - val_loss: 1.0536 - val_acc: 0.7303\n",
      "Epoch 164/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0815 - acc: 0.7122 - val_loss: 1.0468 - val_acc: 0.7251\n",
      "Epoch 165/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0743 - acc: 0.7246 - val_loss: 1.0412 - val_acc: 0.7512\n",
      "Epoch 166/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0703 - acc: 0.7270 - val_loss: 1.0396 - val_acc: 0.6930\n",
      "Epoch 167/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.0673 - acc: 0.7264 - val_loss: 1.0380 - val_acc: 0.7424\n",
      "Epoch 168/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0668 - acc: 0.7283 - val_loss: 1.0354 - val_acc: 0.7552\n",
      "Epoch 169/200\n",
      "400000/400000 [==============================] - 7s - loss: 1.0633 - acc: 0.7242 - val_loss: 1.0342 - val_acc: 0.7191\n",
      "Epoch 170/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0589 - acc: 0.7343 - val_loss: 1.0349 - val_acc: 0.7136\n",
      "Epoch 171/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0568 - acc: 0.7301 - val_loss: 1.0268 - val_acc: 0.7142\n",
      "Epoch 172/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0541 - acc: 0.7349 - val_loss: 1.0222 - val_acc: 0.7525\n",
      "Epoch 173/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0497 - acc: 0.7404 - val_loss: 1.0190 - val_acc: 0.7284\n",
      "Epoch 174/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0478 - acc: 0.7436 - val_loss: 1.0162 - val_acc: 0.7439\n",
      "Epoch 175/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0448 - acc: 0.7448 - val_loss: 1.0101 - val_acc: 0.7455\n",
      "Epoch 176/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0440 - acc: 0.7379 - val_loss: 1.0176 - val_acc: 0.7464\n",
      "Epoch 177/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0421 - acc: 0.7300 - val_loss: 1.0082 - val_acc: 0.7351\n",
      "Epoch 178/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0375 - acc: 0.7424 - val_loss: 1.0083 - val_acc: 0.7423\n",
      "Epoch 179/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0367 - acc: 0.7372 - val_loss: 0.9972 - val_acc: 0.7666\n",
      "Epoch 180/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0317 - acc: 0.7486 - val_loss: 1.0060 - val_acc: 0.7358\n",
      "Epoch 181/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0289 - acc: 0.7546 - val_loss: 1.0002 - val_acc: 0.7692\n",
      "Epoch 182/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0295 - acc: 0.7429 - val_loss: 0.9982 - val_acc: 0.7110\n",
      "Epoch 183/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0276 - acc: 0.7420 - val_loss: 0.9959 - val_acc: 0.7799\n",
      "Epoch 184/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0214 - acc: 0.7597 - val_loss: 0.9935 - val_acc: 0.7198\n",
      "Epoch 185/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0201 - acc: 0.7477 - val_loss: 0.9886 - val_acc: 0.7298\n",
      "Epoch 186/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0187 - acc: 0.7282 - val_loss: 0.9872 - val_acc: 0.7379\n",
      "Epoch 187/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0144 - acc: 0.7498 - val_loss: 0.9806 - val_acc: 0.7893\n",
      "Epoch 188/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0130 - acc: 0.7511 - val_loss: 0.9786 - val_acc: 0.7955\n",
      "Epoch 189/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0111 - acc: 0.7512 - val_loss: 0.9772 - val_acc: 0.7763\n",
      "Epoch 190/200\n",
      "400000/400000 [==============================] - 6s - loss: 1.0064 - acc: 0.7591 - val_loss: 0.9781 - val_acc: 0.7651\n",
      "Epoch 191/200\n",
      "400000/400000 [==============================] - 5s - loss: 1.0033 - acc: 0.7564 - val_loss: 0.9683 - val_acc: 0.7748\n",
      "Epoch 192/200\n",
      "400000/400000 [==============================] - 5s - loss: 0.9987 - acc: 0.7582 - val_loss: 0.9672 - val_acc: 0.7835\n",
      "Epoch 193/200\n",
      "400000/400000 [==============================] - 5s - loss: 0.9968 - acc: 0.7587 - val_loss: 0.9712 - val_acc: 0.7270\n",
      "Epoch 194/200\n",
      "400000/400000 [==============================] - 5s - loss: 0.9984 - acc: 0.7616 - val_loss: 0.9687 - val_acc: 0.7490\n",
      "Epoch 195/200\n",
      "400000/400000 [==============================] - 5s - loss: 0.9941 - acc: 0.7519 - val_loss: 0.9595 - val_acc: 0.7551\n",
      "Epoch 196/200\n",
      "400000/400000 [==============================] - 6s - loss: 0.9911 - acc: 0.7510 - val_loss: 0.9651 - val_acc: 0.7434\n",
      "Epoch 197/200\n",
      "400000/400000 [==============================] - 5s - loss: 0.9917 - acc: 0.7593 - val_loss: 0.9598 - val_acc: 0.7524\n",
      "Epoch 198/200\n",
      "400000/400000 [==============================] - 5s - loss: 0.9890 - acc: 0.7560 - val_loss: 0.9597 - val_acc: 0.7709\n",
      "Epoch 199/200\n",
      "400000/400000 [==============================] - 5s - loss: 0.9887 - acc: 0.7428 - val_loss: 0.9612 - val_acc: 0.7358\n",
      "Epoch 200/200\n",
      "400000/400000 [==============================] - 5s - loss: 0.9861 - acc: 0.7558 - val_loss: 0.9564 - val_acc: 0.7760\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size = batch_sizes, \n",
    "                    epochs = train_epochs, verbose = 1, \n",
    "                    validation_data=(x_test, y_test), \n",
    "                    callbacks=[lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'acc' in h:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1952bfb7eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.956349571619\n",
      "Test accuracy: 0.77603\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do prediction test on random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% prediction accuracy -  78.0\n"
     ]
    }
   ],
   "source": [
    "rr = []\n",
    "numtest = 1000\n",
    "for i in range(0,numtest):\n",
    "    v3 = np.random.randint(0, max_range1)\n",
    "    v4 = np.random.randint(0, max_range2)\n",
    "    v6 = np.array([[v3,v4]]).astype('float32') / max_range1    \n",
    "    v5=np.argmax( model.predict( v6 ) )\n",
    "    r = v5==(v3+v4)\n",
    "    rr.append(r)\n",
    "\n",
    "print(\"% prediction accuracy 1 - \", rr.count(True) / numtest * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% prediction accuracy 2 - 82.34\n"
     ]
    }
   ],
   "source": [
    "rr = []\n",
    "for i in range(0,101):\n",
    "    for j in test_list:\n",
    "        v3 = np.random.randint(0, max_range1)\n",
    "        v4 = j\n",
    "        v6 = np.array([[v3,v4]]).astype('float32') / max_range1    \n",
    "        v5=np.argmax( model.predict( v6 ) )\n",
    "        r = v5==(v3+v4)\n",
    "        rr.append(r)\n",
    "        \n",
    "        # swap v3 and v4 with test list\n",
    "        v3 = j\n",
    "        v4 = np.random.randint(0, max_range1)\n",
    "        v6 = np.array([[v3,v4]]).astype('float32') / max_range1    \n",
    "        v5=np.argmax( model.predict( v6 ) )\n",
    "        r = v5==(v3+v4)\n",
    "        rr.append(r)\n",
    "\n",
    "print(\"% prediction accuracy 2 - {0:2.2f}\".format(rr.count(True) / len(rr) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the accuracy in this case, I have tried to increase the number of training epochs, and can get higher prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
