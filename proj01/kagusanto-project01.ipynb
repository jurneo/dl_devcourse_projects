{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 01 - Addition Two Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief summary to the problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given two numbers $x$ and $y$ with a range from 0 to 100, use deep learning to predict the $z$, the result of adding $x$ and $y$ using classification method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $x = [0\\ldotp\\ldotp 100]$ and $y = [0\\ldotp\\ldotp 100]$, the minimum value of $z$ is **0** and the maximum value is **200**. Therefore, there are **200** number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the problem challenging, we need to take out some integers from the possible value in both $x$ and $y$ lists, for example **28, 51, 73**, from the training data. And use them later in the prediction. The idea is to test if the *trained network* can generalize well to the unseen dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code, starting with the import parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from random import shuffle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_range1, max_range2 = 100, 100 # the maximum value of x and y\n",
    "test_list = [28, 51, 73] # the list of the numbers to be taken out from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = max_range1 + max_range2 + 1 # output class\n",
    "num_data = 500000 # number of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We generate the training data using random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_random_data(seed = 8):\n",
    "    z = []    \n",
    "    seed = 8\n",
    "    np.random.seed(seed)\n",
    "    t = [] # validation data\n",
    "    for i in range(0,num_data):\n",
    "        v1 = np.random.randint(0,max_range1+1)\n",
    "        v2 = np.random.randint(0,max_range2+1)\n",
    "        if v1 in test_list or v2 in test_list:\n",
    "            t.append((v1,v2,v1+v2))\n",
    "            continue\n",
    "        z.append((v1,v2,v1+v2))\n",
    "    z = z + t\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = gen_random_data()\n",
    "\n",
    "# set the dataset as numpy array\n",
    "x=np.array( [v[0:2] for v in z] ).astype('float32')\n",
    "y=np.array( [v[2:][0] for v in z] ).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to normalize the training data to be within the range of $[0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize\n",
    "x = x / max_range1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hot encoding\n",
    "yy=to_categorical(y, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We set 80$\\%$ for training data and 20$\\%$ for validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train and validation data\n",
    "n_train = int( 0.8 * num_data ) \n",
    "x_train = x[0:n_train,]\n",
    "x_test = x[n_train:,]\n",
    "y_train = yy[0:n_train,] \n",
    "y_test = yy[n_train:,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup hyperparameter for dense layer\n",
    "n_input = 2 # x + y -- two input dataset\n",
    "n_hidden_1 = 200 # layer - 1\n",
    "n_hidden_2 = 150 # layer - 2\n",
    "n_hidden_3 = 100 # layer - 3\n",
    "n_hidden_4 = 50 # layer - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Inp = Input(shape=(2,))\n",
    "x = Dense( n_hidden_1, activation = 'relu', name = 'Dense_1')(Inp)\n",
    "x = Dense( n_hidden_2, activation = 'relu', name = 'Dense_2')(x)\n",
    "x = Dense( n_hidden_3, activation = 'relu', name = 'Dense_3')(x)\n",
    "x = Dense( n_hidden_4, activation = 'relu', name = 'Dense_4')(x)\n",
    "output = Dense( n_classes, activation = 'softmax', name = 'Dense_out')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Model(Inp, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 200)               600       \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "Dense_3 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "Dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "Dense_out (Dense)            (None, 201)               10251     \n",
      "=================================================================\n",
      "Total params: 61,151\n",
      "Trainable params: 61,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_epochs = 400\n",
    "batch_sizes = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a function to allow changing the learning rate at different stage of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    if epoch<20:\n",
    "        return 0.002\n",
    "    if epoch<60:\n",
    "        return 0.001\n",
    "    return 0.0005\n",
    "\n",
    "lrate=LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam()\n",
    "model.compile( loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400000 samples, validate on 100000 samples\n",
      "Epoch 1/400\n",
      "400000/400000 [==============================] - 4s - loss: 5.2740 - acc: 0.0081 - val_loss: 5.1965 - val_acc: 0.0094\n",
      "Epoch 2/400\n",
      "400000/400000 [==============================] - 2s - loss: 5.1532 - acc: 0.0092 - val_loss: 5.0674 - val_acc: 0.0091\n",
      "Epoch 3/400\n",
      "400000/400000 [==============================] - 2s - loss: 5.0373 - acc: 0.0090 - val_loss: 4.9424 - val_acc: 0.0113\n",
      "Epoch 4/400\n",
      "400000/400000 [==============================] - 2s - loss: 4.8722 - acc: 0.0108 - val_loss: 4.6889 - val_acc: 0.0162\n",
      "Epoch 5/400\n",
      "400000/400000 [==============================] - 2s - loss: 4.4886 - acc: 0.0289 - val_loss: 4.1501 - val_acc: 0.0332\n",
      "Epoch 6/400\n",
      "400000/400000 [==============================] - 2s - loss: 3.9447 - acc: 0.0557 - val_loss: 3.6487 - val_acc: 0.0884\n",
      "Epoch 7/400\n",
      "400000/400000 [==============================] - 2s - loss: 3.5279 - acc: 0.0913 - val_loss: 3.3115 - val_acc: 0.1229\n",
      "Epoch 8/400\n",
      "400000/400000 [==============================] - 2s - loss: 3.2291 - acc: 0.1284 - val_loss: 3.0520 - val_acc: 0.1466\n",
      "Epoch 9/400\n",
      "400000/400000 [==============================] - 2s - loss: 3.0095 - acc: 0.1468 - val_loss: 2.8882 - val_acc: 0.1396\n",
      "Epoch 10/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.8370 - acc: 0.1769 - val_loss: 2.7144 - val_acc: 0.1886\n",
      "Epoch 11/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.6970 - acc: 0.2035 - val_loss: 2.5951 - val_acc: 0.2199\n",
      "Epoch 12/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.6126 - acc: 0.1953 - val_loss: 2.4996 - val_acc: 0.2318\n",
      "Epoch 13/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.5047 - acc: 0.2350 - val_loss: 2.4278 - val_acc: 0.2711\n",
      "Epoch 14/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.4228 - acc: 0.2588 - val_loss: 2.3520 - val_acc: 0.2778\n",
      "Epoch 15/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.3551 - acc: 0.2759 - val_loss: 2.2759 - val_acc: 0.3105\n",
      "Epoch 16/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.3423 - acc: 0.2237 - val_loss: 2.2754 - val_acc: 0.2319\n",
      "Epoch 17/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.2708 - acc: 0.2755 - val_loss: 2.1987 - val_acc: 0.3294\n",
      "Epoch 18/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.2039 - acc: 0.3201 - val_loss: 2.1419 - val_acc: 0.3174\n",
      "Epoch 19/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.1695 - acc: 0.3111 - val_loss: 2.1082 - val_acc: 0.3161\n",
      "Epoch 20/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.1396 - acc: 0.3118 - val_loss: 2.2104 - val_acc: 0.2029\n",
      "Epoch 21/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.1127 - acc: 0.3058 - val_loss: 2.0431 - val_acc: 0.3444\n",
      "Epoch 22/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.0464 - acc: 0.3813 - val_loss: 1.9794 - val_acc: 0.4354\n",
      "Epoch 23/400\n",
      "400000/400000 [==============================] - 2s - loss: 2.0070 - acc: 0.4398 - val_loss: 1.9623 - val_acc: 0.4413\n",
      "Epoch 24/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.9825 - acc: 0.4675 - val_loss: 1.9362 - val_acc: 0.4569\n",
      "Epoch 25/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.9604 - acc: 0.4724 - val_loss: 1.9180 - val_acc: 0.4840\n",
      "Epoch 26/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.9402 - acc: 0.4845 - val_loss: 1.8958 - val_acc: 0.5193\n",
      "Epoch 27/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.9204 - acc: 0.4907 - val_loss: 1.8805 - val_acc: 0.4981\n",
      "Epoch 28/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.9027 - acc: 0.4776 - val_loss: 1.8605 - val_acc: 0.4676\n",
      "Epoch 29/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.8829 - acc: 0.5074 - val_loss: 1.8442 - val_acc: 0.4759\n",
      "Epoch 30/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.8659 - acc: 0.4985 - val_loss: 1.8226 - val_acc: 0.5083\n",
      "Epoch 31/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.8485 - acc: 0.4982 - val_loss: 1.8076 - val_acc: 0.5099\n",
      "Epoch 32/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.8297 - acc: 0.5132 - val_loss: 1.7871 - val_acc: 0.5132\n",
      "Epoch 33/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.8127 - acc: 0.5047 - val_loss: 1.7671 - val_acc: 0.5266\n",
      "Epoch 34/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.7966 - acc: 0.5003 - val_loss: 1.7567 - val_acc: 0.5142\n",
      "Epoch 35/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.7795 - acc: 0.4955 - val_loss: 1.7362 - val_acc: 0.5042\n",
      "Epoch 36/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.7617 - acc: 0.5157 - val_loss: 1.7201 - val_acc: 0.5174\n",
      "Epoch 37/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.7455 - acc: 0.5335 - val_loss: 1.7050 - val_acc: 0.5355\n",
      "Epoch 38/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.7345 - acc: 0.5172 - val_loss: 1.6944 - val_acc: 0.5489\n",
      "Epoch 39/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.7251 - acc: 0.4991 - val_loss: 1.6898 - val_acc: 0.4934\n",
      "Epoch 40/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.7026 - acc: 0.5119 - val_loss: 1.6506 - val_acc: 0.5798\n",
      "Epoch 41/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.6888 - acc: 0.5355 - val_loss: 1.6373 - val_acc: 0.5585\n",
      "Epoch 42/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.6748 - acc: 0.5202 - val_loss: 1.6333 - val_acc: 0.5516\n",
      "Epoch 43/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.6637 - acc: 0.5242 - val_loss: 1.6242 - val_acc: 0.5084\n",
      "Epoch 44/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.6461 - acc: 0.5455 - val_loss: 1.6001 - val_acc: 0.5775\n",
      "Epoch 45/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.6363 - acc: 0.5384 - val_loss: 1.5897 - val_acc: 0.5355\n",
      "Epoch 46/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.6253 - acc: 0.5282 - val_loss: 1.5859 - val_acc: 0.5427\n",
      "Epoch 47/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.6061 - acc: 0.5583 - val_loss: 1.5835 - val_acc: 0.5038\n",
      "Epoch 48/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.6021 - acc: 0.5236 - val_loss: 1.5677 - val_acc: 0.4985\n",
      "Epoch 49/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5859 - acc: 0.5622 - val_loss: 1.5603 - val_acc: 0.5390\n",
      "Epoch 50/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5804 - acc: 0.5387 - val_loss: 1.5432 - val_acc: 0.5343\n",
      "Epoch 51/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5746 - acc: 0.5281 - val_loss: 1.5426 - val_acc: 0.5337\n",
      "Epoch 52/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5597 - acc: 0.5325 - val_loss: 1.5203 - val_acc: 0.5430\n",
      "Epoch 53/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5446 - acc: 0.5567 - val_loss: 1.4943 - val_acc: 0.6120\n",
      "Epoch 54/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5288 - acc: 0.5663 - val_loss: 1.4803 - val_acc: 0.5822\n",
      "Epoch 55/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5146 - acc: 0.5896 - val_loss: 1.4702 - val_acc: 0.6144\n",
      "Epoch 56/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5053 - acc: 0.6034 - val_loss: 1.4624 - val_acc: 0.6304\n",
      "Epoch 57/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.4984 - acc: 0.5931 - val_loss: 1.4642 - val_acc: 0.5708\n",
      "Epoch 58/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5078 - acc: 0.5564 - val_loss: 1.4642 - val_acc: 0.5581\n",
      "Epoch 59/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5030 - acc: 0.5528 - val_loss: 1.4516 - val_acc: 0.6085\n",
      "Epoch 60/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.5005 - acc: 0.5516 - val_loss: 1.4555 - val_acc: 0.6021\n",
      "Epoch 61/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.4518 - acc: 0.6631 - val_loss: 1.4073 - val_acc: 0.6923\n",
      "Epoch 62/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.4301 - acc: 0.7109 - val_loss: 1.3897 - val_acc: 0.7188\n",
      "Epoch 63/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.4200 - acc: 0.7221 - val_loss: 1.3841 - val_acc: 0.7510\n",
      "Epoch 64/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.4130 - acc: 0.7336 - val_loss: 1.3765 - val_acc: 0.7287\n",
      "Epoch 65/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.4074 - acc: 0.7217 - val_loss: 1.3721 - val_acc: 0.7593\n",
      "Epoch 66/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.4027 - acc: 0.7332 - val_loss: 1.3685 - val_acc: 0.7389\n",
      "Epoch 67/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3973 - acc: 0.7346 - val_loss: 1.3597 - val_acc: 0.7528\n",
      "Epoch 68/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3918 - acc: 0.7452 - val_loss: 1.3564 - val_acc: 0.7620\n",
      "Epoch 69/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3873 - acc: 0.7465 - val_loss: 1.3498 - val_acc: 0.7862\n",
      "Epoch 70/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3816 - acc: 0.7475 - val_loss: 1.3491 - val_acc: 0.7440\n",
      "Epoch 71/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3769 - acc: 0.7376 - val_loss: 1.3402 - val_acc: 0.7503\n",
      "Epoch 72/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3725 - acc: 0.7295 - val_loss: 1.3371 - val_acc: 0.7101\n",
      "Epoch 73/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3686 - acc: 0.7203 - val_loss: 1.3320 - val_acc: 0.7552\n",
      "Epoch 74/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3624 - acc: 0.7350 - val_loss: 1.3257 - val_acc: 0.7253\n",
      "Epoch 75/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3562 - acc: 0.7332 - val_loss: 1.3227 - val_acc: 0.7268\n",
      "Epoch 76/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3512 - acc: 0.7322 - val_loss: 1.3178 - val_acc: 0.7483\n",
      "Epoch 77/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3489 - acc: 0.7324 - val_loss: 1.3121 - val_acc: 0.7400\n",
      "Epoch 78/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3424 - acc: 0.7313 - val_loss: 1.3059 - val_acc: 0.7631\n",
      "Epoch 79/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3375 - acc: 0.7415 - val_loss: 1.3031 - val_acc: 0.7484\n",
      "Epoch 80/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3321 - acc: 0.7476 - val_loss: 1.2951 - val_acc: 0.7363\n",
      "Epoch 81/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3271 - acc: 0.7345 - val_loss: 1.2932 - val_acc: 0.7328\n",
      "Epoch 82/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3232 - acc: 0.7286 - val_loss: 1.2890 - val_acc: 0.7593\n",
      "Epoch 83/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3187 - acc: 0.7336 - val_loss: 1.2886 - val_acc: 0.7229\n",
      "Epoch 84/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3138 - acc: 0.7316 - val_loss: 1.2782 - val_acc: 0.7370\n",
      "Epoch 85/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3074 - acc: 0.7340 - val_loss: 1.2688 - val_acc: 0.7942\n",
      "Epoch 86/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.3021 - acc: 0.7481 - val_loss: 1.2659 - val_acc: 0.7522\n",
      "Epoch 87/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2982 - acc: 0.7397 - val_loss: 1.2676 - val_acc: 0.7373\n",
      "Epoch 88/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2947 - acc: 0.7362 - val_loss: 1.2588 - val_acc: 0.7518\n",
      "Epoch 89/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2886 - acc: 0.7430 - val_loss: 1.2507 - val_acc: 0.7881\n",
      "Epoch 90/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2844 - acc: 0.7404 - val_loss: 1.2471 - val_acc: 0.7586\n",
      "Epoch 91/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2791 - acc: 0.7476 - val_loss: 1.2434 - val_acc: 0.7515\n",
      "Epoch 92/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2761 - acc: 0.7413 - val_loss: 1.2358 - val_acc: 0.7769\n",
      "Epoch 93/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2694 - acc: 0.7519 - val_loss: 1.2342 - val_acc: 0.7751\n",
      "Epoch 94/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2665 - acc: 0.7482 - val_loss: 1.2266 - val_acc: 0.7558\n",
      "Epoch 95/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2606 - acc: 0.7473 - val_loss: 1.2223 - val_acc: 0.7568\n",
      "Epoch 96/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2561 - acc: 0.7479 - val_loss: 1.2268 - val_acc: 0.7497\n",
      "Epoch 97/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2519 - acc: 0.7504 - val_loss: 1.2240 - val_acc: 0.7376\n",
      "Epoch 98/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2494 - acc: 0.7576 - val_loss: 1.2134 - val_acc: 0.7814\n",
      "Epoch 99/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2415 - acc: 0.7597 - val_loss: 1.2153 - val_acc: 0.7431\n",
      "Epoch 100/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2395 - acc: 0.7409 - val_loss: 1.2024 - val_acc: 0.7410\n",
      "Epoch 101/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2358 - acc: 0.7337 - val_loss: 1.2060 - val_acc: 0.7269\n",
      "Epoch 102/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2322 - acc: 0.7307 - val_loss: 1.1927 - val_acc: 0.7588\n",
      "Epoch 103/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2247 - acc: 0.7427 - val_loss: 1.1869 - val_acc: 0.7756\n",
      "Epoch 104/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2221 - acc: 0.7418 - val_loss: 1.1869 - val_acc: 0.7454\n",
      "Epoch 105/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2176 - acc: 0.7462 - val_loss: 1.1813 - val_acc: 0.7398\n",
      "Epoch 106/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2115 - acc: 0.7390 - val_loss: 1.1741 - val_acc: 0.7621\n",
      "Epoch 107/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2058 - acc: 0.7525 - val_loss: 1.1721 - val_acc: 0.7418\n",
      "Epoch 108/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.2010 - acc: 0.7573 - val_loss: 1.1645 - val_acc: 0.7825\n",
      "Epoch 109/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1986 - acc: 0.7521 - val_loss: 1.1587 - val_acc: 0.8270\n",
      "Epoch 110/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1948 - acc: 0.7436 - val_loss: 1.1585 - val_acc: 0.7737\n",
      "Epoch 111/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1896 - acc: 0.7445 - val_loss: 1.1600 - val_acc: 0.7490\n",
      "Epoch 112/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1846 - acc: 0.7603 - val_loss: 1.1483 - val_acc: 0.7992\n",
      "Epoch 113/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1817 - acc: 0.7612 - val_loss: 1.1523 - val_acc: 0.7744\n",
      "Epoch 114/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1782 - acc: 0.7668 - val_loss: 1.1378 - val_acc: 0.7744\n",
      "Epoch 115/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1706 - acc: 0.7731 - val_loss: 1.1369 - val_acc: 0.7931\n",
      "Epoch 116/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1688 - acc: 0.7746 - val_loss: 1.1304 - val_acc: 0.7710\n",
      "Epoch 117/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1627 - acc: 0.7603 - val_loss: 1.1248 - val_acc: 0.8044\n",
      "Epoch 118/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1594 - acc: 0.7725 - val_loss: 1.1280 - val_acc: 0.7839\n",
      "Epoch 119/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1566 - acc: 0.7702 - val_loss: 1.1189 - val_acc: 0.7923\n",
      "Epoch 120/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1499 - acc: 0.7868 - val_loss: 1.1148 - val_acc: 0.7975\n",
      "Epoch 121/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1491 - acc: 0.7736 - val_loss: 1.1167 - val_acc: 0.7906\n",
      "Epoch 122/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1454 - acc: 0.7690 - val_loss: 1.1075 - val_acc: 0.7937\n",
      "Epoch 123/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1419 - acc: 0.7670 - val_loss: 1.1101 - val_acc: 0.7809\n",
      "Epoch 124/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1391 - acc: 0.7542 - val_loss: 1.1022 - val_acc: 0.7792\n",
      "Epoch 125/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1328 - acc: 0.7760 - val_loss: 1.0990 - val_acc: 0.7444\n",
      "Epoch 126/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1290 - acc: 0.7507 - val_loss: 1.0980 - val_acc: 0.7487\n",
      "Epoch 127/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1288 - acc: 0.7553 - val_loss: 1.0893 - val_acc: 0.7743\n",
      "Epoch 128/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1224 - acc: 0.7644 - val_loss: 1.0867 - val_acc: 0.7562\n",
      "Epoch 129/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1205 - acc: 0.7638 - val_loss: 1.0789 - val_acc: 0.7969\n",
      "Epoch 130/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1148 - acc: 0.7734 - val_loss: 1.0813 - val_acc: 0.7788\n",
      "Epoch 131/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1101 - acc: 0.7730 - val_loss: 1.0771 - val_acc: 0.7907\n",
      "Epoch 132/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1064 - acc: 0.7675 - val_loss: 1.0839 - val_acc: 0.7679\n",
      "Epoch 133/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1035 - acc: 0.7706 - val_loss: 1.0656 - val_acc: 0.7750\n",
      "Epoch 134/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.1006 - acc: 0.7712 - val_loss: 1.0641 - val_acc: 0.7942\n",
      "Epoch 135/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0967 - acc: 0.7703 - val_loss: 1.0663 - val_acc: 0.7748\n",
      "Epoch 136/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0937 - acc: 0.7703 - val_loss: 1.0627 - val_acc: 0.7757\n",
      "Epoch 137/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0911 - acc: 0.7636 - val_loss: 1.0575 - val_acc: 0.7881\n",
      "Epoch 138/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0854 - acc: 0.7782 - val_loss: 1.0415 - val_acc: 0.8007\n",
      "Epoch 139/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0794 - acc: 0.7800 - val_loss: 1.0508 - val_acc: 0.8102\n",
      "Epoch 140/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0782 - acc: 0.7766 - val_loss: 1.0376 - val_acc: 0.7875\n",
      "Epoch 141/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0725 - acc: 0.7832 - val_loss: 1.0468 - val_acc: 0.7386\n",
      "Epoch 142/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0720 - acc: 0.7710 - val_loss: 1.0343 - val_acc: 0.7972\n",
      "Epoch 143/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0666 - acc: 0.7745 - val_loss: 1.0312 - val_acc: 0.8097\n",
      "Epoch 144/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0630 - acc: 0.7811 - val_loss: 1.0298 - val_acc: 0.7827\n",
      "Epoch 145/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0631 - acc: 0.7772 - val_loss: 1.0276 - val_acc: 0.7656\n",
      "Epoch 146/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0577 - acc: 0.7762 - val_loss: 1.0268 - val_acc: 0.7905\n",
      "Epoch 147/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0550 - acc: 0.7714 - val_loss: 1.0245 - val_acc: 0.7967\n",
      "Epoch 148/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0502 - acc: 0.7867 - val_loss: 1.0161 - val_acc: 0.8144\n",
      "Epoch 149/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0458 - acc: 0.7860 - val_loss: 1.0113 - val_acc: 0.8093\n",
      "Epoch 150/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0441 - acc: 0.7742 - val_loss: 1.0082 - val_acc: 0.7979\n",
      "Epoch 151/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0393 - acc: 0.7783 - val_loss: 1.0109 - val_acc: 0.7749\n",
      "Epoch 152/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0399 - acc: 0.7749 - val_loss: 0.9985 - val_acc: 0.7950\n",
      "Epoch 153/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0370 - acc: 0.7683 - val_loss: 0.9939 - val_acc: 0.7679\n",
      "Epoch 154/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0278 - acc: 0.7905 - val_loss: 0.9999 - val_acc: 0.7992\n",
      "Epoch 155/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0271 - acc: 0.7866 - val_loss: 0.9926 - val_acc: 0.7893\n",
      "Epoch 156/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0237 - acc: 0.7838 - val_loss: 0.9847 - val_acc: 0.7621\n",
      "Epoch 157/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0224 - acc: 0.7723 - val_loss: 0.9916 - val_acc: 0.8041\n",
      "Epoch 158/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0198 - acc: 0.7797 - val_loss: 0.9830 - val_acc: 0.8121\n",
      "Epoch 159/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0133 - acc: 0.7872 - val_loss: 0.9791 - val_acc: 0.8121\n",
      "Epoch 160/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0104 - acc: 0.7851 - val_loss: 0.9725 - val_acc: 0.7954\n",
      "Epoch 161/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0104 - acc: 0.7830 - val_loss: 0.9762 - val_acc: 0.8312\n",
      "Epoch 162/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0048 - acc: 0.7970 - val_loss: 0.9702 - val_acc: 0.7797\n",
      "Epoch 163/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0023 - acc: 0.7863 - val_loss: 0.9635 - val_acc: 0.8022\n",
      "Epoch 164/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9997 - acc: 0.7866 - val_loss: 0.9659 - val_acc: 0.8057\n",
      "Epoch 165/400\n",
      "400000/400000 [==============================] - 2s - loss: 1.0005 - acc: 0.7906 - val_loss: 0.9586 - val_acc: 0.8117\n",
      "Epoch 166/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9937 - acc: 0.7844 - val_loss: 0.9579 - val_acc: 0.8070\n",
      "Epoch 167/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9912 - acc: 0.7933 - val_loss: 0.9623 - val_acc: 0.8054\n",
      "Epoch 168/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9885 - acc: 0.7861 - val_loss: 0.9657 - val_acc: 0.7864\n",
      "Epoch 169/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9880 - acc: 0.7855 - val_loss: 0.9493 - val_acc: 0.8016\n",
      "Epoch 170/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9828 - acc: 0.7958 - val_loss: 0.9429 - val_acc: 0.8219\n",
      "Epoch 171/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9784 - acc: 0.7999 - val_loss: 0.9468 - val_acc: 0.7781\n",
      "Epoch 172/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9752 - acc: 0.7944 - val_loss: 0.9387 - val_acc: 0.7944\n",
      "Epoch 173/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9724 - acc: 0.7896 - val_loss: 0.9406 - val_acc: 0.8146\n",
      "Epoch 174/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9706 - acc: 0.7923 - val_loss: 0.9356 - val_acc: 0.8123\n",
      "Epoch 175/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9685 - acc: 0.7878 - val_loss: 0.9397 - val_acc: 0.8343\n",
      "Epoch 176/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9690 - acc: 0.7943 - val_loss: 0.9333 - val_acc: 0.8087\n",
      "Epoch 177/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9640 - acc: 0.7798 - val_loss: 0.9238 - val_acc: 0.8342\n",
      "Epoch 178/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9579 - acc: 0.8134 - val_loss: 0.9233 - val_acc: 0.8314\n",
      "Epoch 179/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9569 - acc: 0.8010 - val_loss: 0.9241 - val_acc: 0.8145\n",
      "Epoch 180/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9523 - acc: 0.8050 - val_loss: 0.9229 - val_acc: 0.8197\n",
      "Epoch 181/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9516 - acc: 0.8112 - val_loss: 0.9164 - val_acc: 0.8147\n",
      "Epoch 182/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9483 - acc: 0.7997 - val_loss: 0.9163 - val_acc: 0.7874\n",
      "Epoch 183/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9459 - acc: 0.7993 - val_loss: 0.9175 - val_acc: 0.7871\n",
      "Epoch 184/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9429 - acc: 0.8057 - val_loss: 0.9054 - val_acc: 0.8134\n",
      "Epoch 185/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9367 - acc: 0.8137 - val_loss: 0.9041 - val_acc: 0.8122\n",
      "Epoch 186/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9364 - acc: 0.8018 - val_loss: 0.8974 - val_acc: 0.8223\n",
      "Epoch 187/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9342 - acc: 0.7975 - val_loss: 0.9023 - val_acc: 0.8028\n",
      "Epoch 188/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9321 - acc: 0.8071 - val_loss: 0.9056 - val_acc: 0.7974\n",
      "Epoch 189/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9313 - acc: 0.7991 - val_loss: 0.8926 - val_acc: 0.8160\n",
      "Epoch 190/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9283 - acc: 0.7942 - val_loss: 0.8972 - val_acc: 0.8242\n",
      "Epoch 191/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9243 - acc: 0.7981 - val_loss: 0.8922 - val_acc: 0.8079\n",
      "Epoch 192/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9176 - acc: 0.8077 - val_loss: 0.8854 - val_acc: 0.8178\n",
      "Epoch 193/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9167 - acc: 0.8084 - val_loss: 0.8953 - val_acc: 0.8004\n",
      "Epoch 194/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9177 - acc: 0.7989 - val_loss: 0.8779 - val_acc: 0.8373\n",
      "Epoch 195/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9112 - acc: 0.8127 - val_loss: 0.8718 - val_acc: 0.8222\n",
      "Epoch 196/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9083 - acc: 0.8052 - val_loss: 0.8718 - val_acc: 0.8227\n",
      "Epoch 197/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9065 - acc: 0.8152 - val_loss: 0.8684 - val_acc: 0.8264\n",
      "Epoch 198/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9068 - acc: 0.8050 - val_loss: 0.8758 - val_acc: 0.8080\n",
      "Epoch 199/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.9043 - acc: 0.8045 - val_loss: 0.8677 - val_acc: 0.8126\n",
      "Epoch 200/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8989 - acc: 0.8142 - val_loss: 0.8638 - val_acc: 0.8389\n",
      "Epoch 201/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8987 - acc: 0.8101 - val_loss: 0.8572 - val_acc: 0.8422\n",
      "Epoch 202/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8939 - acc: 0.8157 - val_loss: 0.8553 - val_acc: 0.8117\n",
      "Epoch 203/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8949 - acc: 0.8056 - val_loss: 0.8588 - val_acc: 0.8099\n",
      "Epoch 204/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8960 - acc: 0.7976 - val_loss: 0.8623 - val_acc: 0.8311\n",
      "Epoch 205/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8926 - acc: 0.8012 - val_loss: 0.8555 - val_acc: 0.8103\n",
      "Epoch 206/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8864 - acc: 0.8096 - val_loss: 0.8501 - val_acc: 0.8033\n",
      "Epoch 207/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8853 - acc: 0.8114 - val_loss: 0.8486 - val_acc: 0.7985\n",
      "Epoch 208/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8837 - acc: 0.7971 - val_loss: 0.8454 - val_acc: 0.8212\n",
      "Epoch 209/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8818 - acc: 0.8132 - val_loss: 0.8446 - val_acc: 0.8223\n",
      "Epoch 210/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8779 - acc: 0.8018 - val_loss: 0.8428 - val_acc: 0.8092\n",
      "Epoch 211/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8782 - acc: 0.8052 - val_loss: 0.8382 - val_acc: 0.8388\n",
      "Epoch 212/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8701 - acc: 0.8223 - val_loss: 0.8324 - val_acc: 0.8434\n",
      "Epoch 213/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8697 - acc: 0.8192 - val_loss: 0.8328 - val_acc: 0.8146\n",
      "Epoch 214/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8705 - acc: 0.8122 - val_loss: 0.8374 - val_acc: 0.8325\n",
      "Epoch 215/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8658 - acc: 0.8208 - val_loss: 0.8345 - val_acc: 0.8207\n",
      "Epoch 216/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8634 - acc: 0.8147 - val_loss: 0.8333 - val_acc: 0.8404\n",
      "Epoch 217/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8640 - acc: 0.8174 - val_loss: 0.8406 - val_acc: 0.7998\n",
      "Epoch 218/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8644 - acc: 0.8067 - val_loss: 0.8303 - val_acc: 0.8106\n",
      "Epoch 219/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8563 - acc: 0.8123 - val_loss: 0.8310 - val_acc: 0.8164\n",
      "Epoch 220/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8576 - acc: 0.8107 - val_loss: 0.8169 - val_acc: 0.8641\n",
      "Epoch 221/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8534 - acc: 0.8188 - val_loss: 0.8241 - val_acc: 0.8398\n",
      "Epoch 222/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8512 - acc: 0.8210 - val_loss: 0.8215 - val_acc: 0.8007\n",
      "Epoch 223/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8478 - acc: 0.8160 - val_loss: 0.8240 - val_acc: 0.8384\n",
      "Epoch 224/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8470 - acc: 0.8120 - val_loss: 0.8170 - val_acc: 0.7877\n",
      "Epoch 225/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8478 - acc: 0.8053 - val_loss: 0.8085 - val_acc: 0.8319\n",
      "Epoch 226/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8446 - acc: 0.8054 - val_loss: 0.8065 - val_acc: 0.8429\n",
      "Epoch 227/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8438 - acc: 0.8186 - val_loss: 0.8086 - val_acc: 0.8469\n",
      "Epoch 228/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8377 - acc: 0.8217 - val_loss: 0.8067 - val_acc: 0.8459\n",
      "Epoch 229/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8400 - acc: 0.8157 - val_loss: 0.8012 - val_acc: 0.8274\n",
      "Epoch 230/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8361 - acc: 0.8136 - val_loss: 0.8075 - val_acc: 0.7955\n",
      "Epoch 231/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8347 - acc: 0.8165 - val_loss: 0.8016 - val_acc: 0.8260\n",
      "Epoch 232/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8285 - acc: 0.8194 - val_loss: 0.7920 - val_acc: 0.8235\n",
      "Epoch 233/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8248 - acc: 0.8240 - val_loss: 0.7956 - val_acc: 0.8318\n",
      "Epoch 234/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8241 - acc: 0.8372 - val_loss: 0.7895 - val_acc: 0.8672\n",
      "Epoch 235/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8242 - acc: 0.8300 - val_loss: 0.7951 - val_acc: 0.8253\n",
      "Epoch 236/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8232 - acc: 0.8215 - val_loss: 0.7854 - val_acc: 0.8381\n",
      "Epoch 237/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8189 - acc: 0.8289 - val_loss: 0.7842 - val_acc: 0.8271\n",
      "Epoch 238/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8169 - acc: 0.8253 - val_loss: 0.7805 - val_acc: 0.8376\n",
      "Epoch 239/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8133 - acc: 0.8353 - val_loss: 0.7778 - val_acc: 0.8313\n",
      "Epoch 240/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8117 - acc: 0.8361 - val_loss: 0.7758 - val_acc: 0.8315\n",
      "Epoch 241/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8131 - acc: 0.8246 - val_loss: 0.7752 - val_acc: 0.8348\n",
      "Epoch 242/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8100 - acc: 0.8294 - val_loss: 0.7720 - val_acc: 0.8446\n",
      "Epoch 243/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8059 - acc: 0.8334 - val_loss: 0.7721 - val_acc: 0.8485\n",
      "Epoch 244/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8076 - acc: 0.8314 - val_loss: 0.7814 - val_acc: 0.8221\n",
      "Epoch 245/400\n",
      "400000/400000 [==============================] - 4s - loss: 0.8074 - acc: 0.8180 - val_loss: 0.7746 - val_acc: 0.8431\n",
      "Epoch 246/400\n",
      "400000/400000 [==============================] - 3s - loss: 0.8062 - acc: 0.8175 - val_loss: 0.7708 - val_acc: 0.8243\n",
      "Epoch 247/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.8027 - acc: 0.8260 - val_loss: 0.7674 - val_acc: 0.8398\n",
      "Epoch 248/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7996 - acc: 0.8276 - val_loss: 0.7699 - val_acc: 0.8695\n",
      "Epoch 249/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7987 - acc: 0.8274 - val_loss: 0.7606 - val_acc: 0.8390\n",
      "Epoch 250/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7941 - acc: 0.8248 - val_loss: 0.7597 - val_acc: 0.8234\n",
      "Epoch 251/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7928 - acc: 0.8293 - val_loss: 0.7603 - val_acc: 0.7992\n",
      "Epoch 252/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7896 - acc: 0.8269 - val_loss: 0.7543 - val_acc: 0.8475\n",
      "Epoch 253/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7890 - acc: 0.8342 - val_loss: 0.7546 - val_acc: 0.8701\n",
      "Epoch 254/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7850 - acc: 0.8525 - val_loss: 0.7527 - val_acc: 0.8531\n",
      "Epoch 255/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7828 - acc: 0.8437 - val_loss: 0.7481 - val_acc: 0.8608\n",
      "Epoch 256/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7855 - acc: 0.8385 - val_loss: 0.7574 - val_acc: 0.8324\n",
      "Epoch 257/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7843 - acc: 0.8341 - val_loss: 0.7474 - val_acc: 0.8378\n",
      "Epoch 258/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7845 - acc: 0.8377 - val_loss: 0.7588 - val_acc: 0.8295\n",
      "Epoch 259/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7838 - acc: 0.8266 - val_loss: 0.7495 - val_acc: 0.8678\n",
      "Epoch 260/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7790 - acc: 0.8399 - val_loss: 0.7530 - val_acc: 0.8565\n",
      "Epoch 261/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7768 - acc: 0.8349 - val_loss: 0.7406 - val_acc: 0.8363\n",
      "Epoch 262/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7712 - acc: 0.8254 - val_loss: 0.7337 - val_acc: 0.8497\n",
      "Epoch 263/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7720 - acc: 0.8312 - val_loss: 0.7399 - val_acc: 0.8332\n",
      "Epoch 264/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7673 - acc: 0.8503 - val_loss: 0.7331 - val_acc: 0.8591\n",
      "Epoch 265/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7641 - acc: 0.8448 - val_loss: 0.7351 - val_acc: 0.8727\n",
      "Epoch 266/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7610 - acc: 0.8512 - val_loss: 0.7312 - val_acc: 0.8529\n",
      "Epoch 267/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7614 - acc: 0.8463 - val_loss: 0.7282 - val_acc: 0.8184\n",
      "Epoch 268/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7591 - acc: 0.8425 - val_loss: 0.7206 - val_acc: 0.8530\n",
      "Epoch 269/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7564 - acc: 0.8425 - val_loss: 0.7229 - val_acc: 0.8778\n",
      "Epoch 270/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7614 - acc: 0.8421 - val_loss: 0.7277 - val_acc: 0.8406\n",
      "Epoch 271/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7633 - acc: 0.8310 - val_loss: 0.7302 - val_acc: 0.8389\n",
      "Epoch 272/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7594 - acc: 0.8284 - val_loss: 0.7218 - val_acc: 0.8623\n",
      "Epoch 273/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7531 - acc: 0.8412 - val_loss: 0.7214 - val_acc: 0.8357\n",
      "Epoch 274/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7510 - acc: 0.8457 - val_loss: 0.7198 - val_acc: 0.8432\n",
      "Epoch 275/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7532 - acc: 0.8400 - val_loss: 0.7286 - val_acc: 0.8603\n",
      "Epoch 276/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7572 - acc: 0.8278 - val_loss: 0.7305 - val_acc: 0.8180\n",
      "Epoch 277/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7558 - acc: 0.8230 - val_loss: 0.7194 - val_acc: 0.8261\n",
      "Epoch 278/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7555 - acc: 0.8261 - val_loss: 0.7167 - val_acc: 0.8309\n",
      "Epoch 279/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7494 - acc: 0.8268 - val_loss: 0.7099 - val_acc: 0.8672\n",
      "Epoch 280/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7469 - acc: 0.8356 - val_loss: 0.7148 - val_acc: 0.8290\n",
      "Epoch 281/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7425 - acc: 0.8411 - val_loss: 0.7059 - val_acc: 0.8288\n",
      "Epoch 282/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7389 - acc: 0.8351 - val_loss: 0.7015 - val_acc: 0.8578\n",
      "Epoch 283/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7381 - acc: 0.8457 - val_loss: 0.7025 - val_acc: 0.8534\n",
      "Epoch 284/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7374 - acc: 0.8391 - val_loss: 0.7126 - val_acc: 0.8465\n",
      "Epoch 285/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7350 - acc: 0.8491 - val_loss: 0.7045 - val_acc: 0.8441\n",
      "Epoch 286/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7326 - acc: 0.8422 - val_loss: 0.7008 - val_acc: 0.8545\n",
      "Epoch 287/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7305 - acc: 0.8511 - val_loss: 0.6922 - val_acc: 0.8757\n",
      "Epoch 288/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7262 - acc: 0.8460 - val_loss: 0.6976 - val_acc: 0.8558\n",
      "Epoch 289/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7266 - acc: 0.8505 - val_loss: 0.6887 - val_acc: 0.9023\n",
      "Epoch 290/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7250 - acc: 0.8516 - val_loss: 0.6938 - val_acc: 0.8410\n",
      "Epoch 291/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7232 - acc: 0.8490 - val_loss: 0.6861 - val_acc: 0.8559\n",
      "Epoch 292/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7204 - acc: 0.8483 - val_loss: 0.6838 - val_acc: 0.8730\n",
      "Epoch 293/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7177 - acc: 0.8508 - val_loss: 0.6924 - val_acc: 0.8633\n",
      "Epoch 294/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7208 - acc: 0.8492 - val_loss: 0.6867 - val_acc: 0.8528\n",
      "Epoch 295/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7189 - acc: 0.8444 - val_loss: 0.6786 - val_acc: 0.8839\n",
      "Epoch 296/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7168 - acc: 0.8547 - val_loss: 0.6859 - val_acc: 0.8523\n",
      "Epoch 297/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7148 - acc: 0.8473 - val_loss: 0.6796 - val_acc: 0.8498\n",
      "Epoch 298/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7128 - acc: 0.8501 - val_loss: 0.6770 - val_acc: 0.8497\n",
      "Epoch 299/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7165 - acc: 0.8395 - val_loss: 0.6867 - val_acc: 0.8551\n",
      "Epoch 300/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7114 - acc: 0.8462 - val_loss: 0.6736 - val_acc: 0.8808\n",
      "Epoch 301/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7107 - acc: 0.8441 - val_loss: 0.6798 - val_acc: 0.8618\n",
      "Epoch 302/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7116 - acc: 0.8408 - val_loss: 0.6850 - val_acc: 0.8358\n",
      "Epoch 303/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7094 - acc: 0.8423 - val_loss: 0.6743 - val_acc: 0.8742\n",
      "Epoch 304/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7092 - acc: 0.8488 - val_loss: 0.6798 - val_acc: 0.8359\n",
      "Epoch 305/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7086 - acc: 0.8392 - val_loss: 0.6719 - val_acc: 0.8449\n",
      "Epoch 306/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7029 - acc: 0.8432 - val_loss: 0.6666 - val_acc: 0.8565\n",
      "Epoch 307/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.7013 - acc: 0.8453 - val_loss: 0.6721 - val_acc: 0.8409\n",
      "Epoch 308/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6982 - acc: 0.8471 - val_loss: 0.6665 - val_acc: 0.8687\n",
      "Epoch 309/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6991 - acc: 0.8509 - val_loss: 0.6639 - val_acc: 0.8563\n",
      "Epoch 310/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6979 - acc: 0.8441 - val_loss: 0.6692 - val_acc: 0.8530\n",
      "Epoch 311/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6935 - acc: 0.8478 - val_loss: 0.6660 - val_acc: 0.8676\n",
      "Epoch 312/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6936 - acc: 0.8603 - val_loss: 0.6598 - val_acc: 0.8501\n",
      "Epoch 313/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6935 - acc: 0.8464 - val_loss: 0.6664 - val_acc: 0.8400\n",
      "Epoch 314/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6923 - acc: 0.8458 - val_loss: 0.6631 - val_acc: 0.8783\n",
      "Epoch 315/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6907 - acc: 0.8548 - val_loss: 0.6541 - val_acc: 0.8680\n",
      "Epoch 316/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6889 - acc: 0.8475 - val_loss: 0.6524 - val_acc: 0.8646\n",
      "Epoch 317/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6878 - acc: 0.8499 - val_loss: 0.6606 - val_acc: 0.8484\n",
      "Epoch 318/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6882 - acc: 0.8470 - val_loss: 0.6527 - val_acc: 0.8738\n",
      "Epoch 319/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6855 - acc: 0.8479 - val_loss: 0.6491 - val_acc: 0.8620\n",
      "Epoch 320/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6854 - acc: 0.8512 - val_loss: 0.6501 - val_acc: 0.8779\n",
      "Epoch 321/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6841 - acc: 0.8501 - val_loss: 0.6516 - val_acc: 0.8472\n",
      "Epoch 322/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6811 - acc: 0.8467 - val_loss: 0.6450 - val_acc: 0.8508\n",
      "Epoch 323/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6801 - acc: 0.8533 - val_loss: 0.6481 - val_acc: 0.8638\n",
      "Epoch 324/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6774 - acc: 0.8574 - val_loss: 0.6494 - val_acc: 0.8543\n",
      "Epoch 325/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6732 - acc: 0.8635 - val_loss: 0.6372 - val_acc: 0.8815\n",
      "Epoch 326/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6726 - acc: 0.8584 - val_loss: 0.6363 - val_acc: 0.8657\n",
      "Epoch 327/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6741 - acc: 0.8581 - val_loss: 0.6514 - val_acc: 0.8625\n",
      "Epoch 328/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6746 - acc: 0.8615 - val_loss: 0.6382 - val_acc: 0.8418\n",
      "Epoch 329/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6724 - acc: 0.8489 - val_loss: 0.6513 - val_acc: 0.8368\n",
      "Epoch 330/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6727 - acc: 0.8496 - val_loss: 0.6419 - val_acc: 0.8463\n",
      "Epoch 331/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6723 - acc: 0.8492 - val_loss: 0.6458 - val_acc: 0.8423\n",
      "Epoch 332/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6778 - acc: 0.8350 - val_loss: 0.6411 - val_acc: 0.8553\n",
      "Epoch 333/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6699 - acc: 0.8546 - val_loss: 0.6441 - val_acc: 0.8740\n",
      "Epoch 334/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6690 - acc: 0.8541 - val_loss: 0.6311 - val_acc: 0.8408\n",
      "Epoch 335/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6639 - acc: 0.8544 - val_loss: 0.6304 - val_acc: 0.8712\n",
      "Epoch 336/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6614 - acc: 0.8626 - val_loss: 0.6279 - val_acc: 0.8713\n",
      "Epoch 337/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6611 - acc: 0.8584 - val_loss: 0.6297 - val_acc: 0.8588\n",
      "Epoch 338/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6590 - acc: 0.8657 - val_loss: 0.6259 - val_acc: 0.8624\n",
      "Epoch 339/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6615 - acc: 0.8553 - val_loss: 0.6365 - val_acc: 0.8507\n",
      "Epoch 340/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6635 - acc: 0.8450 - val_loss: 0.6389 - val_acc: 0.8424\n",
      "Epoch 341/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6596 - acc: 0.8503 - val_loss: 0.6216 - val_acc: 0.8608\n",
      "Epoch 342/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6528 - acc: 0.8575 - val_loss: 0.6181 - val_acc: 0.8738\n",
      "Epoch 343/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6526 - acc: 0.8643 - val_loss: 0.6286 - val_acc: 0.8642\n",
      "Epoch 344/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6562 - acc: 0.8534 - val_loss: 0.6182 - val_acc: 0.8671\n",
      "Epoch 345/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6491 - acc: 0.8593 - val_loss: 0.6135 - val_acc: 0.8628\n",
      "Epoch 346/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6475 - acc: 0.8660 - val_loss: 0.6180 - val_acc: 0.8878\n",
      "Epoch 347/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6463 - acc: 0.8710 - val_loss: 0.6170 - val_acc: 0.8528\n",
      "Epoch 348/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6443 - acc: 0.8628 - val_loss: 0.6207 - val_acc: 0.8676\n",
      "Epoch 349/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6477 - acc: 0.8455 - val_loss: 0.6284 - val_acc: 0.8295\n",
      "Epoch 350/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6541 - acc: 0.8616 - val_loss: 0.6284 - val_acc: 0.8626\n",
      "Epoch 351/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6459 - acc: 0.8664 - val_loss: 0.6193 - val_acc: 0.8532\n",
      "Epoch 352/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6422 - acc: 0.8523 - val_loss: 0.6158 - val_acc: 0.8456\n",
      "Epoch 353/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6446 - acc: 0.8526 - val_loss: 0.6145 - val_acc: 0.8519\n",
      "Epoch 354/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6426 - acc: 0.8566 - val_loss: 0.6092 - val_acc: 0.8876\n",
      "Epoch 355/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6397 - acc: 0.8627 - val_loss: 0.6044 - val_acc: 0.8715\n",
      "Epoch 356/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6361 - acc: 0.8540 - val_loss: 0.6155 - val_acc: 0.8460\n",
      "Epoch 357/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6415 - acc: 0.8522 - val_loss: 0.6106 - val_acc: 0.8707\n",
      "Epoch 358/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6410 - acc: 0.8710 - val_loss: 0.6128 - val_acc: 0.8800\n",
      "Epoch 359/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6432 - acc: 0.8551 - val_loss: 0.6062 - val_acc: 0.8614\n",
      "Epoch 360/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6408 - acc: 0.8524 - val_loss: 0.6124 - val_acc: 0.8481\n",
      "Epoch 361/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6357 - acc: 0.8514 - val_loss: 0.6028 - val_acc: 0.8526\n",
      "Epoch 362/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6350 - acc: 0.8576 - val_loss: 0.5973 - val_acc: 0.8698\n",
      "Epoch 363/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6314 - acc: 0.8634 - val_loss: 0.6077 - val_acc: 0.8696\n",
      "Epoch 364/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6358 - acc: 0.8582 - val_loss: 0.6005 - val_acc: 0.8674\n",
      "Epoch 365/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6289 - acc: 0.8599 - val_loss: 0.6020 - val_acc: 0.8651\n",
      "Epoch 366/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6295 - acc: 0.8566 - val_loss: 0.6019 - val_acc: 0.8494\n",
      "Epoch 367/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6297 - acc: 0.8531 - val_loss: 0.6046 - val_acc: 0.8619\n",
      "Epoch 368/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6296 - acc: 0.8581 - val_loss: 0.5983 - val_acc: 0.8689\n",
      "Epoch 369/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6240 - acc: 0.8601 - val_loss: 0.5910 - val_acc: 0.8801\n",
      "Epoch 370/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6210 - acc: 0.8714 - val_loss: 0.5939 - val_acc: 0.8726\n",
      "Epoch 371/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6210 - acc: 0.8663 - val_loss: 0.5854 - val_acc: 0.8892\n",
      "Epoch 372/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6171 - acc: 0.8718 - val_loss: 0.5849 - val_acc: 0.8578\n",
      "Epoch 373/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6191 - acc: 0.8659 - val_loss: 0.5925 - val_acc: 0.8735\n",
      "Epoch 374/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6254 - acc: 0.8484 - val_loss: 0.5908 - val_acc: 0.8494\n",
      "Epoch 375/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6196 - acc: 0.8466 - val_loss: 0.5894 - val_acc: 0.8345\n",
      "Epoch 376/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6171 - acc: 0.8585 - val_loss: 0.5859 - val_acc: 0.8381\n",
      "Epoch 377/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6147 - acc: 0.8628 - val_loss: 0.5868 - val_acc: 0.8690\n",
      "Epoch 378/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6107 - acc: 0.8762 - val_loss: 0.5765 - val_acc: 0.8656\n",
      "Epoch 379/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6117 - acc: 0.8705 - val_loss: 0.5821 - val_acc: 0.8930\n",
      "Epoch 380/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6118 - acc: 0.8647 - val_loss: 0.5776 - val_acc: 0.8790\n",
      "Epoch 381/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6082 - acc: 0.8714 - val_loss: 0.5902 - val_acc: 0.8696\n",
      "Epoch 382/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6127 - acc: 0.8556 - val_loss: 0.5792 - val_acc: 0.8778\n",
      "Epoch 383/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6081 - acc: 0.8736 - val_loss: 0.5794 - val_acc: 0.8649\n",
      "Epoch 384/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6073 - acc: 0.8704 - val_loss: 0.5728 - val_acc: 0.8850\n",
      "Epoch 385/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6037 - acc: 0.8669 - val_loss: 0.5778 - val_acc: 0.8776\n",
      "Epoch 386/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6015 - acc: 0.8696 - val_loss: 0.5668 - val_acc: 0.9190\n",
      "Epoch 387/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6002 - acc: 0.8761 - val_loss: 0.5709 - val_acc: 0.8982\n",
      "Epoch 388/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6010 - acc: 0.8837 - val_loss: 0.5774 - val_acc: 0.8725\n",
      "Epoch 389/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6033 - acc: 0.8642 - val_loss: 0.5626 - val_acc: 0.9135\n",
      "Epoch 390/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.5951 - acc: 0.8856 - val_loss: 0.5619 - val_acc: 0.8994\n",
      "Epoch 391/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.5962 - acc: 0.8796 - val_loss: 0.5639 - val_acc: 0.8630\n",
      "Epoch 392/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.5983 - acc: 0.8820 - val_loss: 0.5681 - val_acc: 0.8688\n",
      "Epoch 393/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.5965 - acc: 0.8680 - val_loss: 0.5663 - val_acc: 0.8747\n",
      "Epoch 394/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6008 - acc: 0.8694 - val_loss: 0.5702 - val_acc: 0.8824\n",
      "Epoch 395/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6036 - acc: 0.8632 - val_loss: 0.5764 - val_acc: 0.8669\n",
      "Epoch 396/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6042 - acc: 0.8558 - val_loss: 0.5762 - val_acc: 0.8726\n",
      "Epoch 397/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6004 - acc: 0.8544 - val_loss: 0.5649 - val_acc: 0.8582\n",
      "Epoch 398/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6018 - acc: 0.8553 - val_loss: 0.5735 - val_acc: 0.8689\n",
      "Epoch 399/400\n",
      "400000/400000 [==============================] - 2s - loss: 0.6037 - acc: 0.8567 - val_loss: 0.5732 - val_acc: 0.8394\n",
      "Epoch 400/400\n",
      "400000/400000 [==============================] - 3s - loss: 0.6032 - acc: 0.8495 - val_loss: 0.5654 - val_acc: 0.8673\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size = batch_sizes, \n",
    "                    epochs = train_epochs, verbose = 1, \n",
    "                    validation_data=(x_test, y_test), \n",
    "                    callbacks=[lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'acc' in h:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naVJQSinlpUlBKaWU1/8HDGLpEnQIxVwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16b5c790208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.56541998786\n",
      "Test accuracy: 0.86727\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do prediction test on random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% prediction accuracy 1 -  87.9\n"
     ]
    }
   ],
   "source": [
    "rr = []\n",
    "numtest = 1000\n",
    "for i in range(0,numtest):\n",
    "    v3 = np.random.randint(0, max_range1)\n",
    "    v4 = np.random.randint(0, max_range2)\n",
    "    v6 = np.array([[v3,v4]]).astype('float32') / max_range1    \n",
    "    v5=np.argmax( model.predict( v6 ) )\n",
    "    r = v5==(v3+v4)\n",
    "    rr.append(r)\n",
    "\n",
    "print(\"% prediction accuracy 1 - \", rr.count(True) / numtest * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction accuracy from random dataset: **87.9$\\%$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% prediction accuracy 2 - 88.12\n"
     ]
    }
   ],
   "source": [
    "rr = []\n",
    "for i in range(0,101):\n",
    "    for j in test_list:\n",
    "        v3 = np.random.randint(0, max_range1)\n",
    "        v4 = j\n",
    "        v6 = np.array([[v3,v4]]).astype('float32') / max_range1    \n",
    "        v5=np.argmax( model.predict( v6 ) )\n",
    "        r = v5==(v3+v4)\n",
    "        rr.append(r)\n",
    "        \n",
    "        # swap v3 and v4 with test list\n",
    "        v3 = j\n",
    "        v4 = np.random.randint(0, max_range1)\n",
    "        v6 = np.array([[v3,v4]]).astype('float32') / max_range1    \n",
    "        v5=np.argmax( model.predict( v6 ) )\n",
    "        r = v5==(v3+v4)\n",
    "        rr.append(r)\n",
    "\n",
    "print(\"% prediction accuracy 2 - {0:2.2f}\".format(rr.count(True) / len(rr) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction accuracy from test list dataset: **88.12$\\%$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the accuracy in this case, I have tried to increase the number of training epochs, and can get higher prediction accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
